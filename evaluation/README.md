# Evaluation folder
Here you can find all files for generating the statistics and evaluating the findings of the automatic matching algorithms.

- `align_by_hand.py`: GUI for aligning articles manually
- `alignment_statistics.py`: Prints the number of automatically aligned sentences produced by `main_alignment.py`
- `check_groundtruth.py`: 
- `eval_matches_auto.py`: Generates latex tables for statistics on all automatic alignment strategy and similarity measure combinations (Avg. number of matches / similarity)
- `eval_on_gt.py`: Compares hand aligned articles (i.e. from `align_by_hand.py`) with the automatic alignment methods, generated by `main_matching.py`.
- `eval_properties.py`: Generates generall statistics of all articles
- `evaluate_evaluations.py`: Evaluates results of automatic alignment methods (by `main_matching.py`) with the results of `evaluate.py`. Not needed.
- `evaluate_progress.py`: Basically the same as `evaluate_evaluations.py`. Both files are not needed
- `evaluate.py`: Provides basic utilities to evaluate the automatic alignments by hand. It shows two aligned sentences and asks the users to determine if it is correctly/incorrectly aligned or unclear. This does not provide context and is hence not recommended for general usage.